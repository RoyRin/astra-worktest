{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak-to-Strong Generalization with In-Context Learning\n",
    "\n",
    "This notebook demonstrates weak-to-strong (W2S) generalization using in-context learning on the TruthfulQA dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We investigate whether strong models can learn from weak model predictions via few-shot learning, testing:\n",
    "- **Weak models**: Llama-3.1-8B, Llama-3.1-70B, Qwen-2.5-72B\n",
    "- **Strong models**: Llama-3.1-8B, Llama-3.1-70B, Qwen-2.5-72B\n",
    "- **Few-shot examples**: 0, 2, 4, 8, 16, 32, 64\n",
    "\n",
    "## Experiments\n",
    "\n",
    "1. **Baseline Evaluation**: Zero-shot performance of each model\n",
    "2. **Weak-to-Strong Learning**: Train strong models with weak labels\n",
    "3. **Gold Label Baseline**: Strong models with gold labels (upper bound)\n",
    "4. **Analysis**: Response matching, PGR, and correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Add code directory to path\n",
    "project_root = Path.cwd().parent\n",
    "code_dir = project_root / \"code\"\n",
    "sys.path.insert(0, str(code_dir))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Code directory: {code_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute Baselines\n",
    "\n",
    "First, we evaluate each model's zero-shot performance on TruthfulQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import baseline evaluation code\n",
    "from baseline_evals import baselines\n",
    "from dataset_utils import load_truthful_qa\n",
    "from api_utils import OpenRouterAPI\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading TruthfulQA dataset...\")\n",
    "test_set, train_set = load_truthful_qa(test_size=200, random_seed=42)\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "\n",
    "# Define models to evaluate\n",
    "models = [\n",
    "    \"meta-llama/llama-3.1-8b-instruct\",\n",
    "    \"meta-llama/llama-3.1-70b-instruct\",\n",
    "    \"qwen/qwen-2.5-72b-instruct\"\n",
    "]\n",
    "\n",
    "# Initialize API (update path to your API key)\n",
    "api_key_path = project_root / \"SECRETS\" / \"api.key\"\n",
    "api = OpenRouterAPI(api_key_path=str(api_key_path))\n",
    "\n",
    "# Results directory\n",
    "results_dir = project_root / \"results\" / \"weak_labels\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nResults will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline evaluations\n",
    "print(\"=\"*80)\n",
    "print(\"Running baseline evaluations...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\nEvaluating {model}...\")\n",
    "    \n",
    "    # Run baseline\n",
    "    result = baselines.run_baseline(\n",
    "        model=model,\n",
    "        test_set=test_set,\n",
    "        api=api,\n",
    "        output_dir=results_dir\n",
    "    )\n",
    "    \n",
    "    baseline_results[model] = result\n",
    "    \n",
    "    print(f\"  Accuracy: {result['results']['accuracy']*100:.2f}%\")\n",
    "    print(f\"  Cost: ${result['cost']['total_cost']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline evaluations complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gold Label Sweep\n",
    "\n",
    "Evaluate models with varying numbers of gold-labeled few-shot examples (upper bound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gold label sweep code\n",
    "from sweep_gold_label_counts import sweep_number_of_gold_labels\n",
    "\n",
    "# Define shot counts to test\n",
    "num_shots_list = [0, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Running gold label sweep...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Models: {models}\")\n",
    "print(f\"Shot counts: {num_shots_list}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gold label sweep\n",
    "gold_results = sweep_number_of_gold_labels.run_gold_sweep(\n",
    "    models=models,\n",
    "    num_shots_list=num_shots_list,\n",
    "    test_set=test_set,\n",
    "    train_set=train_set,\n",
    "    api=api,\n",
    "    output_dir=project_root / \"results\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Gold label sweep complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weak-to-Strong Learning\n",
    "\n",
    "Train strong models using labels from weak models with varying numbers of few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weak-to-strong sweep code\n",
    "from sweep_weak_label_counts import sweep_number_of_weak_labels\n",
    "\n",
    "# Define model pairs (weak -> strong)\n",
    "model_pairs = [\n",
    "    (\"meta-llama/llama-3.1-8b-instruct\", \"meta-llama/llama-3.1-8b-instruct\"),  # Self-supervision\n",
    "    (\"meta-llama/llama-3.1-8b-instruct\", \"meta-llama/llama-3.1-70b-instruct\"),\n",
    "    (\"meta-llama/llama-3.1-8b-instruct\", \"qwen/qwen-2.5-72b-instruct\"),\n",
    "    (\"meta-llama/llama-3.1-70b-instruct\", \"meta-llama/llama-3.1-70b-instruct\"),  # Self-supervision\n",
    "    (\"meta-llama/llama-3.1-70b-instruct\", \"qwen/qwen-2.5-72b-instruct\"),\n",
    "    (\"qwen/qwen-2.5-72b-instruct\", \"qwen/qwen-2.5-72b-instruct\"),  # Self-supervision\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Running weak-to-strong sweep...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model pairs: {len(model_pairs)}\")\n",
    "print(f\"Shot counts: {num_shots_list}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run weak-to-strong sweep\n",
    "w2s_results = sweep_number_of_weak_labels.run_weak_sweep(\n",
    "    model_pairs=model_pairs,\n",
    "    num_shots_list=num_shots_list,\n",
    "    test_set=test_set,\n",
    "    train_set=train_set,\n",
    "    api=api,\n",
    "    output_dir=project_root / \"results\",\n",
    "    weak_labels_dir=results_dir\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Weak-to-strong sweep complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting: Accuracy vs. Few-Shot Examples\n",
    "\n",
    "Visualize W2S performance compared to gold baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting code\n",
    "from sweep_weak_label_counts import plot_weak_label_count_sweep\n",
    "\n",
    "# Get latest results\n",
    "results_dir_main = project_root / \"results\"\n",
    "weak_sweep_files = list(results_dir_main.glob(\"weak_sweep_*.json\"))\n",
    "gold_sweep_files = list(results_dir_main.glob(\"gold_sweep_*.json\"))\n",
    "\n",
    "if weak_sweep_files:\n",
    "    latest_weak = max(weak_sweep_files, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"Loading weak sweep results: {latest_weak.name}\")\n",
    "    with open(latest_weak, 'r') as f:\n",
    "        weak_results = json.load(f)\n",
    "else:\n",
    "    print(\"No weak sweep results found\")\n",
    "    weak_results = None\n",
    "\n",
    "if gold_sweep_files:\n",
    "    latest_gold = max(gold_sweep_files, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"Loading gold sweep results: {latest_gold.name}\")\n",
    "    with open(latest_gold, 'r') as f:\n",
    "        gold_results_loaded = json.load(f)\n",
    "else:\n",
    "    print(\"No gold sweep results found\")\n",
    "    gold_results_loaded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy sweep\n",
    "if weak_results:\n",
    "    fig, ax = plot_weak_label_count_sweep.plot_weak_sweep(\n",
    "        weak_results, \n",
    "        gold_results_loaded\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also save\n",
    "    plots_dir = project_root / \"results\" / \"plots\"\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    fig.savefig(plots_dir / f\"weak_sweep_accuracy_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to {plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot self-supervision cases\n",
    "if weak_results:\n",
    "    fig, ax = plot_weak_label_count_sweep.plot_self_supervision(\n",
    "        weak_results,\n",
    "        gold_results_loaded\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    fig.savefig(plots_dir / f\"weak_sweep_self_supervision_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to {plots_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Gap Recovered (PGR)\n",
    "\n",
    "Calculate and visualize PGR: how much of the performance gap between weak and strong models is recovered by W2S learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PGR with zero-shot ceiling\n",
    "if weak_results:\n",
    "    fig, ax = plot_weak_label_count_sweep.plot_pgr(\n",
    "        weak_results,\n",
    "        baseline_results_dir=results_dir\n",
    "    )\n",
    "    if fig:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fig.savefig(plots_dir / f\"pgr_zero_shot_ceiling_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PGR with gold-label ceiling (more fair comparison)\n",
    "if weak_results and gold_results_loaded:\n",
    "    fig, ax = plot_weak_label_count_sweep.plot_pgr_with_gold_ceiling(\n",
    "        weak_results,\n",
    "        baseline_results_dir=results_dir,\n",
    "        gold_results=gold_results_loaded\n",
    "    )\n",
    "    if fig:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fig.savefig(plots_dir / f\"pgr_gold_ceiling_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {plots_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Matching Analysis\n",
    "\n",
    "Analyze how W2S predictions align with weak and strong model predictions.\n",
    "\n",
    "### Grid Plots: All W2S Pairs\n",
    "\n",
    "Generate comprehensive 3×3 grid visualizations showing all model pair combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid plotting script\n",
    "print(\"Generating response matching grid plots...\")\n",
    "print(\"This will create 3 comprehensive grid figures:\")\n",
    "print(\"  1. Response matching (all questions)\")\n",
    "print(\"  2. Response matching (when weak wrong)\")\n",
    "print(\"  3. W2S correctness conditioned on strong performance\")\n",
    "print()\n",
    "\n",
    "# Import and run grid plotting\n",
    "import sys\n",
    "sys.path.insert(0, str(code_dir / \"plotting_and_analysis\"))\n",
    "from plot_all_response_matching_grid import main as plot_grid_main\n",
    "\n",
    "plot_grid_main()\n",
    "\n",
    "print(\"\\nGrid plots saved to results/plots/response_matching/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis\n",
    "\n",
    "Analyze correlations between weak, strong, and W2S predictions for all model pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import correlation analysis\n",
    "from analyze_w2s_correlations import analyze_w2s_pair, load_results\n",
    "\n",
    "# Analyze all W2S pairs\n",
    "print(\"=\"*80)\n",
    "print(\"Correlation Analysis for All W2S Pairs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for weak_model, strong_model in model_pairs:\n",
    "    weak_slug = weak_model.replace('/', '_')\n",
    "    strong_slug = strong_model.replace('/', '_')\n",
    "    \n",
    "    w2s_pattern = f\"w2s_{weak_slug}_to_{strong_slug}_*.json\"\n",
    "    w2s_files = list(results_dir.glob(w2s_pattern))\n",
    "    \n",
    "    if w2s_files:\n",
    "        w2s_file = max(w2s_files, key=lambda p: p.stat().st_mtime)\n",
    "        \n",
    "        weak_baseline_file = results_dir / f\"{weak_slug}_baseline.json\"\n",
    "        strong_baseline_file = results_dir / f\"{strong_slug}_baseline.json\"\n",
    "        \n",
    "        if weak_baseline_file.exists() and strong_baseline_file.exists():\n",
    "            print(f\"\\n{weak_model} → {strong_model}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Run analysis\n",
    "            analyze_w2s_pair(\n",
    "                w2s_file=w2s_file,\n",
    "                weak_baseline_file=weak_baseline_file,\n",
    "                strong_baseline_file=strong_baseline_file,\n",
    "                ground_truth=[q.answer for q in test_set]\n",
    "            )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of all results\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBaseline Accuracies (Zero-Shot):\")\n",
    "print(\"-\" * 80)\n",
    "for model, result in baseline_results.items():\n",
    "    model_name = model.split('/')[-1]\n",
    "    acc = result['results']['accuracy'] * 100\n",
    "    print(f\"  {model_name:30s}: {acc:6.2f}%\")\n",
    "\n",
    "if weak_results:\n",
    "    print(\"\\nWeak-to-Strong Results (64 shots):\")\n",
    "    print(\"-\" * 80)\n",
    "    for pair_key, pair_info in weak_results[\"model_pairs\"].items():\n",
    "        weak_name = pair_info[\"weak_model_name\"]\n",
    "        strong_name = pair_info[\"strong_model_name\"]\n",
    "        \n",
    "        # Get 64-shot result\n",
    "        for result in pair_info[\"results\"]:\n",
    "            if result[\"num_few_shot\"] == 64:\n",
    "                acc = result[\"accuracy\"] * 100\n",
    "                print(f\"  {weak_name:15s} → {strong_name:15s}: {acc:6.2f}%\")\n",
    "                break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All results saved to:\", project_root / \"results\")\n",
    "print(\"All plots saved to:\", project_root / \"results\" / \"plots\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Baseline Performance**: Zero-shot capabilities of each model\n",
    "2. **Gold Label Upper Bound**: Performance with ground truth few-shot examples\n",
    "3. **Weak-to-Strong Learning**: Strong models can learn from weak labels via ICL\n",
    "4. **Performance Gap Recovered**: Quantifies how much of the weak→strong gap is closed\n",
    "5. **Response Matching**: Shows when W2S aligns with weak vs. strong predictions\n",
    "6. **Conditional Analysis**: W2S ability to fix strong errors and preserve strong successes\n",
    "\n",
    "Key findings can be extracted from the plots and correlation analyses above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

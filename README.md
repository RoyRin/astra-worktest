# TASK

Task Description:
* On the TruthfulQA dataset (which the starter code below loads for you), choose a sweep of models on the OpenRouter API that perform at different levels on the task. 
    We recommend starting with Llama 3.1 8B, Llama 3.1 70B and Llama 3.1 405B.

Prompt the largest model with few-shot examples using answers generated by one of the weaker models in the sweep.
You should determine a Performance Gap Recovered (PGR) metric by comparing the performance of the weaker model when prompted with "gold" few-shot examples, the performance of the strong model when prompted with "weak-labelled" few-shot examples, and the performance of the strong model when prompted with "gold" few-shot examples.





# Extra follow ups:

If you have time for additional followup, you could consider exploring any of the following ideas, or others that youâ€™re more interested in!

* how PGR varies with different gaps between strong and weak models
* how the PGR changes with the number of examples in the few-shot prompt.
* how to tweak the few-shot prompt to improve the PGR (e.g. giving the strong model some indication of the strength of the labels)
* how the PGR changes if you use chain of thought
* Trying more tasks or models (https://openrouter.ai/models; remember you have a budget so be aware of model costs)

